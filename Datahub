Architecture:

There are many ways to solve a problem with SAP Datahub, and often that's the biggest challenge is sorting out which way
makes the most sense. 

In the scoping process, immediately start putting words into pipeline and operators. Don't wait, it will help things make sense and
allow asking the right questions. 

Installation:
	Prerequisites for installing SAP Vora: Supported Infrastructure, Container, and Hadoop Platforms
	https://launchpad.support.sap.com/#/notes/2213226
	Vora 2.0 Central release note
	https://launchpad.support.sap.com/#/notes/2523557
	Documentation
	https://help.sap.com/viewer/p/SAP_VORA

Example, Vora accepts string values higher than HANA, so the conversion into Vora might lead to incompatability with HANA
	Fix: 	cast values as a vora view, then move to HANA
		Or use type fixed in the avro injestor
		or Precreate the Vora table with limits
			Type must be a streaming table on create.
	
	Pre creating the Vora table allows more options than the Avro operator. 	
		Setting Primary Keys
		Define Datatypes easily

Best Practices:
	Use folders, and hirearchies you define to seperate pipelines.
	Use labeling on operators inside of pipelines to explain their usage.
	Precreate tables in Vora
		Currently allows you to set primary key or not null constraint where Avro cannot
		
Reusabilty: 
	All pipelines are held in JSON with all operators and config. Get used to exporting these, and reusing configurations.
	Change from graph view to JSON view
	


How to stop a running pipeline via Signal? 
	Create an attribute that switches on end of file from the load, that attribute is pushed through the pipeline and can 
	be acted on by the graph terminator. 


Ports are Dynamic In SAP DATAHUB 2:
	kubectl get services should give you an output of what is running where
Debugging pipelines:
	use the terminal operator:
		Just multiplex out to terminal on all pipelines for debugging.
	Turn on tracing for a pipeline
	
Injesting data into Vora:
	Vora Avro Ingestor is useful in most cases, can create tables, etc.
	Avro Syntax:
		"{ \"name\": \"[TABLE NAME]\", \"type\": \"record\", \"fields\": [{\"name\": \"[COL1]\", \"type\": \"[DATATYPE]\"}, {\"name\": \"[COL2]\", \"type\": \"[DATATYPE]\"}
		Avro syntax here is super exact, extra spaces will cause failure

Golang Operatior - 

package main

import (
    "bufio"
    "bytes"
    "encoding/json"
    "fmt"
    "regexp"
    "strings"
)

var Output func(interface{})
var re = regexp.MustCompile("(.*)\\.(.*)(-.*)?\\.csv")

func main() {
  // no special setup needed    
}

func schemaFromCSV(database string, table string, csv []byte) string {
    r := bufio.NewReaderSize(bytes.NewReader(csv), 255)
    line, _, _ := r.ReadLine()
    columns := strings.Split(string(line), ",")

    fields := make([]map[string]interface{}, len(columns))
    for i := range columns {
        fields[i] = map[string]interface{}{"name": columns[i], "type": "string"}
        if columns[i] != "id" {
            fields[i]["type"] = []string{"null", "string"}
        }
    }
    schema := map[string]interface{}{
        "namespace": database+".cloud.sap",
        "type": "record",
        "name": table,
        "fields": fields,
    }
    
    b, err := json.Marshal(schema)
	if err != nil {
		fmt.Println("error:", err)
	    return ""
	}
	return string(b)
}

// here implement the function with the input port name, so data coming from this input will call this function
func Input(val interface{}){
	message := val.(map[string]interface{})
	
	fmt.Println("ccloud input arrived")
	
	// read contents
	body := message["Body"].([]byte)
	attr := message["Attributes"].(map[string]interface{})
	newAttr := make(map[string]interface{}, len(attr)+2)
	for key := range attr {
		newAttr[key] = attr[key]
	}

	fmt.Println("ccloud: contents read")
	
	// enhance header
    filename_seg := re.FindStringSubmatch(attr["storage.filename"].(string))
	if len(filename_seg) < 3 {
	    fmt.Println("ccloud: ERROR filename cannot be parsed")
	    return
	}
	
	database := filename_seg[1]
	table := filename_seg[2]
    newAttr["vora.record.definition"] = map[string]interface{}{"recName": table}
    newAttr["avro.schema"] = schemaFromCSV(database, table, body)

    fmt.Println("ccloud: output header ", newAttr)

    // build transformed message
	newMessage := make(map[string]interface{}, 3)
	newMessage["Attributes"] = newAttr
	newMessage["Encoding"] = message["Encoding"]
	newMessage["Body"] = body

    // output
    fmt.Println("ccloud output body: ", string(body))
	Output(newMessage)
}
